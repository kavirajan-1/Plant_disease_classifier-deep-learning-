# -*- coding: utf-8 -*-
"""Plant Disease Prediction CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fuNOF734Xgva_wZRe2_SyiMkpXuDRwZ4
"""

# SET SEEDS FOR REPRODUCIBLITY
import random
random.seed(0)

import numpy as np
np.random.seed(0)

import tensorflow as tf
tf.random.set_seed(0)

import os
import json
from zipfile import ZipFile
from PIL import Image

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers,models

!pip install kaggle

kaggle_credentails=json.load(open("kaggle.json"))

#setup kaggle api key as environment variables
os.environ['KAGGLE_USERNAME']=kaggle_credentails["username"]
os.environ['KAGGLE_KEY']=kaggle_credentails["key"]

!kaggle datasets download -d abdallahalidev/plantvillage-dataset

!ls

#Unzip the downloaded dataset
with ZipFile("plantvillage-dataset.zip", 'r') as zip_ref:
  zip_ref.extractall()

print(os.listdir("plantvillage dataset"))


print(len(os.listdir("plantvillage dataset/segmented")))
print(os.listdir("plantvillage dataset/segmented")[:5])

print(len(os.listdir("plantvillage dataset/color")))
print(os.listdir("plantvillage dataset/color")[:5])

print(len(os.listdir("plantvillage dataset/grayscale")))
print(os.listdir("plantvillage dataset/grayscale")[:5])

print(len(os.listdir("plantvillage dataset/color/Grape___healthy")))
print(os.listdir("plantvillage dataset/color/Grape___healthy")[:5])

#Dataset Path
base_dir='plantvillage dataset/color'

image_path='/content/plantvillage dataset/color/Apple___Cedar_apple_rust/025b2b9a-0ec4-4132-96ac-7f2832d0db4a___FREC_C.Rust 3655.JPG'

#Read The Image
img=mping.imread(image_path)

print(img.shape)
#Display The Image
plt.imshow(img)
plt.axis('off') #Turn of axis number
plt.show()

image_path='/content/plantvillage dataset/color/Apple___Cedar_apple_rust/025b2b9a-0ec4-4132-96ac-7f2832d0db4a___FREC_C.Rust 3655.JPG'

#Read The Image
img=mping.imread(image_path)

print(img)

# Image parameters
img_size=224
batch_size=32

#Image data Generator
data_gen=ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2 #use 20% of data for validation
)

#Train Generator
train_generator=data_gen.flow_from_directory(
    base_dir,
    target_size=(img_size,img_size),
    batch_size=batch_size,
    subset='training',
    class_mode='categorical'
)

#Validation Generator
validation_generator=data_gen.flow_from_directory(
    base_dir,
    target_size=(img_size,img_size),
    batch_size=batch_size,
    subset='validation',
    class_mode='categorical'
)

#Model Definition
model=models.Sequential()

model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(img_size,img_size,3)))
model.add(layers.MaxPooling2D(2,2))

model.add(layers.Conv2D(64,(3,3),activation='relu'))
model.add(layers.MaxPooling2D(2,2))

model.add(layers.Flatten())
model.add(layers.Dense(256,activation='relu'))
model.add(layers.Dense(train_generator.num_classes,activation='softmax'))

# model summary
model.summary()

#compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#Training the Model
history=model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,  #Number of steps pre epoch
    epochs=5, #Number of epochs
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // batch_size  #Validation Steps
)

#Model Evaluation
print('Evaluation Model ...')
val_loss,val_accuracy=model.evaluate(validation_generator,steps=validation_generator.samples // batch_size)
print(f"Validation Accuracy :{val_accuracy * 100:.2f}%")

#Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'],loc='upper left')
plt.show()

#plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train','Test'],loc='upper right')
plt.show()

#Function to load and preprocess the image using pillow
def load_and_preprocess_image(image_path,target_size=(224,224)):
  #load image
  img=Image.open(image_path)
  #resize the image
  img=img.resize(target_size)
  #convert the image to a numpy array
  img_array=np.array(img)
  #add batch dimension
  img_array=np.expand_dims(img_array,axis=0)
  #scale the image values to [0,1]
  img_array=img_array.astype('float32')/255.
  return img_array

#Function to predict the class of an image
def predict_image_class(model,image_path,class_indices):
  preprocessed_img=load_and_preprocess_image(image_path)
  predictions=model.predict(preprocessed_img)
  predicted_class_index=np.argmax(predictions,axis=1)[0]
  predicted_class_name=class_indices[predicted_class_index]
  return predicted_class_name

#creating a mapping from class indices to class names
class_indices={v: k for k, v in train_generator.class_indices.items()}

class_indices

#saving the class names asjson file
json.dump(class_indices, open('class_indice.json', 'w'))

#Example Usage
image_path='/content/0199a733-6d3f-4e66-a903-2d1d34457871___JR_FrgE.S 3079.JPG'
#image_path='/content/01a66316-0e98-4d3b-a56f-d78752cd043f___FREC_Scab 3003.JPG'

# Correct the variable name from 'clas_indices' to 'class_indices'
predicted_class_name=predict_image_class(model, image_path, class_indices)

#output the result
print("Predicted Class Name :",predicted_class_name)

model.save('drive/MyDrive/trained_models/plant_disease_prediction_model.h5')

#save the model to google drive
model.save('plant_disease_prediction_model.h5')

"""stream lit"""

! pip install py-localtunnel

! npm install -g localtunnel

! pip install streamlit -q

!curl ipv4.icanhazip.com

! streamlit run main.py & npx localtunnel --port 8501

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# import os
# import json
# from PIL import Image
# 
# import numpy as np
# import tensorflow as tf
# import streamlit as st
# 
# 
# working_dir = os.path.dirname(os.path.abspath(__file__))
# model_path = f"{working_dir}/plant_disease_prediction_model.h5"
# # Load the pre-trained model
# model = tf.keras.models.load_model(model_path)
# 
# # loading the class names
# class_indices = json.load(open(f"{working_dir}/class_indice.json"))
# 
# 
# # Function to Load and Preprocess the Image using Pillow
# def load_and_preprocess_image(image_path, target_size=(224, 224)):
#     # Load the image
#     img = Image.open(image_path)
#     # Resize the image
#     img = img.resize(target_size)
#     # Convert the image to a numpy array
#     img_array = np.array(img)
#     # Add batch dimension
#     img_array = np.expand_dims(img_array, axis=0)
#     # Scale the image values to [0, 1]
#     img_array = img_array.astype('float32') / 255.
#     return img_array
# 
# 
# # Function to Predict the Class of an Image
# def predict_image_class(model, image_path, class_indices):
#     preprocessed_img = load_and_preprocess_image(image_path)
#     predictions = model.predict(preprocessed_img)
#     predicted_class_index = np.argmax(predictions, axis=1)[0]
#     predicted_class_name = class_indices[str(predicted_class_index)]
#     return predicted_class_name
# 
# 
# # Streamlit App
# st.title('ðŸŒ¿â‹…Plant Disease Classifier')
# 
# uploaded_image = st.file_uploader("Upload an image...", type=["jpg", "jpeg", "png"])
# 
# if uploaded_image is not None:
#     image = Image.open(uploaded_image)
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         resized_img = image.resize((150, 150))
#         st.image(resized_img)
# 
#     with col2:
#         if st.button('Classify'):
#             # Preprocess the uploaded image and predict the class
#             prediction = predict_image_class(model, uploaded_image, class_indices)
#             st.success(f'Prediction: {str(prediction)}')